## LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention

**Authors:** Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao

**Published:** 2023-03-28

**Summary:** The LLaMA-Adapter is an efficient fine-tuning method for the LLaMA language model. It introduces only 1.2M learnable parameters to the frozen LLaMA 7B model and takes less than an hour to fine-tune on 8 A100 GPUs. The method uses 52K self-instruct demonstrations and employs learnable adaption prompts and a zero-init attention mechanism with zero gating. This allows the model to adapt to new instructional cues while preserving pre-trained knowledge. The resulting model generates high-quality responses comparable to Alpaca with fully fine-tuned 7B parameters. LLaMA-Adapter can also be extended to multi-modal input, such as images, for image-conditioned LLaMA, which demonstrates superior reasoning capacity on ScienceQA. The code for LLaMA-Adapter is available at https://github.com/ZrrSkywalker/LLaMA-Adapter.