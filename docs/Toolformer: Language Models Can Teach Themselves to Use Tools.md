## Toolformer: Language Models Can Teach Themselves to Use Tools

The Toolformer is a language model that learns to use external tools such as search engines, calculators, and translation systems via simple API calls in a self-supervised way. It is trained on a dataset augmented with API calls, allowing the model to decide when and how to use the available tools. The tools explored in this approach include a question answering system, a calculator, a Wikipedia search engine, a machine translation system, and a calendar. The Toolformer is evaluated on various downstream tasks in a zero-shot setup, and its performance is compared to other baseline models such as GPT-J, GPT-J + CC, OPT, and GPT-3. The results show that the Toolformer can effectively use the available tools to improve its performance on tasks where the tools are helpful, outperforming baseline models in many cases. However, it still lags behind the much larger GPT-3 model in some tasks, suggesting potential areas for future improvement and development. The model's ability to leverage the provided tools only emerges at around 775M parameters, with smaller models achieving similar performance both with and without tools. As models grow in size, their ability to make good use of the provided API improves, maintaining a large gap between predictions with and without API calls even for the biggest models.